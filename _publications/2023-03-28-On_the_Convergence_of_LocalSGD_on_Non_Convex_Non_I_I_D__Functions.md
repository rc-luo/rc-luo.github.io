---
title: "On the Convergence of LocalSGD on Non-Convex Non-I.I.D. Functions"
collection: publications
permalink: /publication/2023-03-28-On_the_Convergence_of_LocalSGD_on_Non_Convex_Non_I_I_D__Functions
excerpt: 'We present a refined analysis of LocalSGD for non-convex non-i.i.d. function.'
date: 2023-03-28
venue: 'In submission'
paperurl: '/files/On_the_Convergence_of_LocalSGD_on_Non_Convex_Non_I_I_D__Functions.pdf'
citation: ''
featured: yes
mark: ''
---

*Abstract.* In federated learning (FL), by taking local steps, LocalSGD is a
strong baseline method with significant communication saving in
non-convex optimization. However, the theoretical analysis of
LocalSGD shows quite the opposite result, in which the rate of
LocalSGD can match the rate of MbSGD (without local steps) only
under very strict conditions.

In this work, we showed (for the first time) that: 1. LocalSGD can benefit from Hessian similarity, which yields an improved conditioning. 2. LocalSGD can converge provably faster than MbSGD for a class of non-convex functions. 3. LocalSGD can converge provably faster than MbSGD without dependency on uniform gradient similarity.


[Download paper here](/files/On_the_Convergence_of_LocalSGD_on_Non_Convex_Non_I_I_D__Functions.pdf)
